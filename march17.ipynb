{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c586b3-4055-4fa1-9dc5-52bcfc22f1d3",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201814b-4e11-417e-85ae-48697b85d76c",
   "metadata": {},
   "source": [
    "the absence of value in a one or more than one row or column it is called as missing values in data set. it is due to incomplete surveys or some technical error.\n",
    "\n",
    "it is important to handle the missing values because it these missing valuse are not handled they can affect the accuracy and reliability of the statistical analysis and machine learning models.\n",
    "\n",
    "Decision Trees: Decision trees can handle missing values by simply ignoring the missing attribute while splitting the node.\n",
    "\n",
    "Random Forest: Random Forest is an ensemble learning algorithm that uses multiple decision trees. It can handle missing values in a similar way as decision trees.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that can handle missing values by using distance measures that ignore missing values.\n",
    "\n",
    "Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing values by using probability estimation based on available data.\n",
    "\n",
    "Support Vector Machines (SVM): SVM can handle missing values by finding the hyperplane that separates the classes using only the available data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97ec04-4a76-4e2e-a1ee-75a05b88d358",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd93915-891b-45b8-941c-44402571c7f7",
   "metadata": {},
   "source": [
    "1. Deletion technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449ba812-6fd3-424e-a053-23d7bbc2959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fbdee3-ab92-47ee-9fbd-7b6b4fda641d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e15399d2-8478-4faa-8199-660181acf329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21fc7641-bff6-4398-900f-87bf2d77a705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962608f6-8f0b-4023-92ea-133f1eccc442",
   "metadata": {},
   "source": [
    "Mean/Mode imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e93e121-eb5c-40c9-87c5-f4fd9a6522c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_age']=df['age'].fillna(df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e793249-93f1-4189-a8cb-9713a6313e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.000000\n",
       "1      38.000000\n",
       "2      26.000000\n",
       "3      35.000000\n",
       "4      35.000000\n",
       "         ...    \n",
       "886    27.000000\n",
       "887    19.000000\n",
       "888    29.699118\n",
       "889    26.000000\n",
       "890    32.000000\n",
       "Name: mean_age, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f112bd-1317-4d5f-aad5-d6c84cccd4b7",
   "metadata": {},
   "source": [
    "Mode imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8391e0d3-78f6-4bcf-9687-d279a10b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mode_age']=df['age'].fillna(df['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a558af-a2a5-46ca-a19e-e7245f4bd5a6",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39677793-01da-41c5-90b8-6bec46cac896",
   "metadata": {},
   "source": [
    "imbalanced dataset is situation where proportion of different classes in a dataset is not equal.\n",
    "\n",
    "issue with imbalanced data is that it can lead to overfitting. Since the training data is imbalanced, the model may learn to overemphasize the dominant class and ignore the minority class. As a result, the model may perform poorly on new data, especially when the proportion of classes in the new data is different from that in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061269ae-cafc-4848-8a1e-3bb9251d24c0",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e50dc-f1f5-4f75-9120-6a916f074b72",
   "metadata": {},
   "source": [
    "upsampling is technique where number of samples are increased to balance the dataset. Up-sampling is usefull when number of samples are not sufficient to provide enough information for a model to learn.\n",
    "\n",
    "on the other hand down-sampling is technique where number of majority class to balance the dataset.his can be useful when the model is overfitting due to the high number of samples in the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c565ba9-2490-4721-8763-24a54ceba46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a2ab119-3db9-4ed4-9491-feb28c257e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_samples=1000\n",
    "class_0_ratio=0.8\n",
    "n_class_0=int(1000*class_0_ratio)\n",
    "n_class_1=n_samples-class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87ddf11b-3119-4761-8a0d-df84bab60342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0=pd.DataFrame({'feature1':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "                      'feature2':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "                      'target':[0]*n_class_0})\n",
    "class_1=pd.DataFrame({'feature1':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "                      'feature2':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "                      'target':[1]*n_class_1})\n",
    "df=pd.concat([class_0,class_1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3f1c4e3-8d49-4e70-9ca0-d67cbd750c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.240970</td>\n",
       "      <td>-0.023158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.312947</td>\n",
       "      <td>0.817158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.848947</td>\n",
       "      <td>-0.114684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.377953</td>\n",
       "      <td>0.151101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.657501</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2.187654</td>\n",
       "      <td>1.035632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>2.688842</td>\n",
       "      <td>1.434644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>2.872073</td>\n",
       "      <td>2.768760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0.821980</td>\n",
       "      <td>3.571835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>2.403432</td>\n",
       "      <td>1.824550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  target\n",
       "0   -1.240970 -0.023158       0\n",
       "1   -0.312947  0.817158       0\n",
       "2   -0.848947 -0.114684       0\n",
       "3    2.377953  0.151101       0\n",
       "4    0.657501  0.364286       0\n",
       "..        ...       ...     ...\n",
       "987  2.187654  1.035632       1\n",
       "890  2.688842  1.434644       1\n",
       "861  2.872073  2.768760       1\n",
       "909  0.821980  3.571835       1\n",
       "819  2.403432  1.824550       1\n",
       "\n",
       "[1600 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upsampling\n",
    "df_majority=df[df['target']==0]\n",
    "df_minority=df[df['target']==1]\n",
    "from sklearn.utils import resample\n",
    "df_minority_upsampled=resample(df_minority,replace=True,n_samples=len(df_majority),random_state=43)\n",
    "df_upsampled=pd.concat([df_majority,df_minority_upsampled])\n",
    "df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7940f2e-eeb9-4466-81b7-f57d6b148bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.709558</td>\n",
       "      <td>1.420166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.659486</td>\n",
       "      <td>0.927048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1.233609</td>\n",
       "      <td>2.877016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>1.691253</td>\n",
       "      <td>2.365602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>1.918255</td>\n",
       "      <td>2.221934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.248838</td>\n",
       "      <td>0.551986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>-0.114125</td>\n",
       "      <td>0.087095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.137109</td>\n",
       "      <td>-0.707814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>-0.917864</td>\n",
       "      <td>0.771613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>0.031060</td>\n",
       "      <td>-0.980190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  target\n",
       "800  0.709558  1.420166       1\n",
       "801  0.659486  0.927048       1\n",
       "802  1.233609  2.877016       1\n",
       "803  1.691253  2.365602       1\n",
       "804  1.918255  2.221934       1\n",
       "..        ...       ...     ...\n",
       "90   0.248838  0.551986       0\n",
       "650 -0.114125  0.087095       0\n",
       "181  0.137109 -0.707814       0\n",
       "790 -0.917864  0.771613       0\n",
       "554  0.031060 -0.980190       0\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downsampling \n",
    "df_majority_downsampled=resample(df_majority,replace=False,n_samples=len(df_minority),random_state=42)\n",
    "df_downsampled=pd.concat([df_minority,df_majority_downsampled])\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb888d1-77cc-4a8c-ae27-41c9f7bb9a88",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f68140-eec5-4e12-b65e-280c92f80d68",
   "metadata": {},
   "source": [
    "The data Augmentation is a technique used in machine learning to increase the size of data set by creation additional training examples from the original data.\n",
    "\n",
    "SMOTE is used to address the problem of imbalanced datasets, where the number of instances of one class is much lower than the other. SMOTE works by generating synthetic examples of the minority class by interpolating between existing instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10aaa1-49e6-4405-b31b-c0eafe9c12df",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278e155-6bbb-46a0-9798-d44831ef0737",
   "metadata": {},
   "source": [
    "Outliers are the data points that are signifficantly different from other datapoints. they may be greater or smaller than other data points.\n",
    "\n",
    "it is essential to handle the outliers because:\n",
    "1. If the outliers are not handled properly, they can significantly skew the results and distort the overall picture of the data.\n",
    "2. outlier can affect statistical meansure such as mean and statnders deviation \n",
    "3. outlier can affect performance of machine learning models which are sensitive to outliers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9a850-11d2-4d6f-b9af-31ddf981ca5b",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878901c8-dd18-49cd-a802-e15210364a5e",
   "metadata": {},
   "source": [
    "There are several techniques to handle the missing data points.\n",
    "1. deleting : in the approach we simply delete the row in which there are one or mode missing data points \n",
    "2. Imputation: missing valuse are replaced with mean,mode or median value \n",
    "3. Model-based imputation: odel-based imputation involves building a statistical model that can be used to estimate missing values. \n",
    "4. Multiple imputation: Multiple imputation is a technique that involves creating multiple imputed datasets based on different plausible values for the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8baa8e-5ae2-4677-afd0-84554446d7a6",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4e171-5e52-4c52-8b1f-7374664cfdb8",
   "metadata": {},
   "source": [
    "Analyze the missingness pattern: One way to determine if there is a pattern to the missing data is to analyze the pattern of missingness. For example, you can examine if certain variables have more missing values than others, if missingness occurs at random times or in specific periods, or if certain groups have more missing values than others.\n",
    "\n",
    "Impute the missing data: Another strategy to determine the pattern of missing data is to impute the missing data using various imputation methods such as mean imputation, median imputation, or multiple imputations. After imputing the data, you can compare the imputed data with the original data to see if there are any systematic differences in the distributions of the imputed and original data.\n",
    "\n",
    "Perform hypothesis testing: You can also use hypothesis testing to determine if the missing data is missing at random or if there is a pattern to the missing data. For example, you can perform a chi-square test of independence to examine if there is a relationship between the missingness of one variable and the missingness of another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca607ce-d29b-450e-8f7c-7b624bca1980",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa827ed0-ea5f-4f17-8685-94cc037fdd85",
   "metadata": {},
   "source": [
    "some stategires to deal with this problem are :\n",
    "    \n",
    "Resample the dataset: One approach to balance the dataset is to oversample the minority class or undersample the majority class. Oversampling can be achieved by duplicating the minority class samples or generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique). Undersampling can be done by randomly removing samples from the majority class. However, this approach may cause a loss of information.\n",
    "\n",
    "Penalize the algorithm: Penalizing the algorithm can be another approach to handling the imbalance. In some algorithms such as Random Forest, XGBoost, and Logistic Regression, you can assign higher weights to the minority class or introduce class weights. This will force the algorithm to pay more attention to the minority class during training.\n",
    "\n",
    "Use ensemble methods: Another approach is to use ensemble methods such as bagging, boosting, or stacking. These methods combine multiple models to improve the overall performance of the model on the imbalanced dataset.\n",
    "\n",
    "Use cross-validation: Cross-validation can be a useful technique to evaluate the performance of your model on an imbalanced dataset. In particular, stratified cross-validation can ensure that each fold contains a representative proportion of the minority class.\n",
    "\n",
    "Consider data augmentation: Data augmentation can be useful in generating more data to balance the dataset. For example, in medical diagnosis projects, you could use data augmentation to create synthetic data points that represent different scenarios or variations in the disease condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2500f-d0ae-44e2-b21b-8e2d874596b2",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c0209-b13c-4870-ac7a-572364ce0b69",
   "metadata": {},
   "source": [
    "Way to balance the dataset are :\n",
    "1. downsampling : It involves randomly deleting the samples from the majority class untill its size is not equal to minority class. \n",
    "2. upsampling  : It involves creating copies of minority class to match the size if minority class with the sie of majority class.\n",
    "3. SMOTE : it creates new synthetic samples for the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c44fb-f182-4229-afb5-e23acaf6ef6b",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ec426-bc84-4908-91aa-8d1e847773ee",
   "metadata": {},
   "source": [
    "most usefull technique here will be upsampling the minority class \n",
    "it can be done by SMOTE which will generate the synthetic samples in minority class.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
