{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f12ffe-a285-4f0a-acd0-acd8c1429d1c",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe83b6-5d81-48b6-af65-2121c94fa4c9",
   "metadata": {},
   "source": [
    "#### The main difference between simple linear regression and multiple linear regression is:\n",
    "- The simple linear regression has one dependent variable and one independent varilable. Where the single independent variable is used for predicting the vlaue of the dependent variable.for example predicting the weight of the person by its height.\n",
    "- And the  multiple linear regression has more than one independent variables and single dependent variable And with the help of multiple independent variables the value of dependent  variables is calculated.prediction the price of the house by its lokation,number of rooms and squre footage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788828a-ba0a-4611-9f87-1817dc60d1f1",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4c743-a5f9-4787-9bc5-276cdcdae0e1",
   "metadata": {},
   "source": [
    "#### Here are the assumptions of linear regression:\n",
    "\n",
    "- Linearity: The relationship between the independent and dependent variables must be linear. This means that the residuals (the difference between the actual values and the predicted values) should be randomly scattered around the line of best fit.\n",
    "- Homoscedasticity: The variance of the residuals should be constant across all values of the independent variable. This means that the residuals should be equally spread out around the line of best fit.\n",
    "- Independence: The residuals should be independent of each other. This means that the residuals from one observation should not affect the residuals from another observation.\n",
    "- Normality: The residuals should be normally distributed. This means that the residuals should be bell-shaped and symmetrical around the mean.\n",
    "#### There are a number of ways to check whether these assumptions hold in a given dataset. Here are some of the most common methods:\n",
    "\n",
    "- Residual plots: Residual plots are a graphical way to check for linearity, homoscedasticity, and independence. A residual plot shows the residuals on the y-axis and the independent variable on the x-axis. If the residuals are randomly scattered around the line of best fit, then the assumption of linearity is met. If the variance of the residuals is constant across all values of the independent variable, then the assumption of homoscedasticity is met. If the residuals are not independent of each other, then the assumption of independence is not met.\n",
    "- Normality tests: There are a number of statistical tests that can be used to check for normality. The most common test is the Shapiro-Wilk test. The Shapiro-Wilk test tests the hypothesis that the residuals are normally distributed. If the p-value of the Shapiro-Wilk test is less than 0.05, then the assumption of normality is not met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3db071-564a-4ed2-aeb0-4133e544c058",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81178121-e6ac-4776-906a-558b4d1ac7c9",
   "metadata": {},
   "source": [
    "#### The Slop and the Intercept are two coefficent that defines best fit line \n",
    "- The slop tells us how much the dependent variable change for every unit vange in the independent variable.\n",
    "- The Intercept tells what is the value of dependent variable when the independent variable is equal to zero\n",
    "\n",
    "\n",
    "- Let's say we are training the model whil will predict the weight of the person based on it's height. Here slop will the us how much change will be there in weight for every unit of change in the height. and the intercept is the value of weight when the height is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35c398-0400-4810-bf29-4ae88ed2ad7e",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d418c-f5e3-41bf-898f-2d417e7a3028",
   "metadata": {},
   "source": [
    "- Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In machine learning, gradient descent is used to train machine learning models\n",
    "- Gradient descent works by starting with a random set of parameters and then iteratively updating the parameters in the direction of the negative gradient of the cost function. The negative gradient of the cost function points in the direction of the steepest descent, so by following the negative gradient, we are moving towards the minimum of the cost function.\n",
    "- The gradient descent algorithm is repeated until the parameters converge to a minimum of the cost function. The number of iterations that the algorithm takes to converge depends on the complexity of the cost function and the learning rate. The learning rate is a hyperparameter that controls the size of the steps that the algorithm takes. A smaller learning rate will take more iterations to converge, but it will be more likely to converge to a global minimum. A larger learning rate will take fewer iterations to converge, but it is more likely to converge to a local minimum.\n",
    "- Let's say we want to train a linear regression model to predict the price of a house based on its square footage. The cost function for this model would be the sum of the squared residuals. The residuals are the difference between the actual price of the house and the predicted price of the house. The gradient of the cost function is a vector that points in the direction of the steepest descent. The gradient descent algorithm would start with a random set of parameters for the linear regression model and then iteratively update the parameters in the direction of the negative gradient of the cost function. The algorithm would repeat until the parameters converge to a minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88315a-1080-43a7-8a69-10432ccae76a",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e265dfe-6748-4f95-b6d8-2cf7ccaf6dbb",
   "metadata": {},
   "source": [
    "- Multuiple linear regression is model that uses the multiple independent varibles to predict the value of the single dependent variable.\n",
    "- y=b0+b1+b2x2+......+b3x3\n",
    "- simple linear regression is special case where there is only one single independent variable.\n",
    "- The main difference between simple and multiple linear regression is multiple regression has multiple independebt variables and single dependent variable whereas, simple linear regression has single independent and single dependent  variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e7b6f-9374-4d49-ae90-bf71ee20eca6",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e0e94-204e-47ae-bfee-2801ace81179",
   "metadata": {},
   "source": [
    "- Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated. This means that the independent variables are redundant and that the model is not able to distinguish between them.\n",
    "- There are a number of ways to detect multicollinearity in a multiple linear regression model. One way is to look at the correlation matrix of the independent variables. If two or more independent variables are highly correlated, then there is likely to be multicollinearity. Another way to detect multicollinearity is to look at the variance inflation factors (VIFs) of the independent variables. The VIFs are a measure of how much the variance of a coefficient is inflated due to multicollinearity. A VIF of 1 indicates that there is no multicollinearity, while a VIF greater than 10 indicates that there is likely to be multicollinearity.\n",
    "\n",
    "- Once multicollinearity has been detected, there are a number of ways to address the issue. One way is to remove one of the correlated independent variables from the model. Another way is to combine the correlated independent variables into a single composite variable. Finally, it is also possible to use a statistical technique called ridge regression to address multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3a4e9-cfbf-4f28-b637-0eaf263ca550",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9add23-1c8c-4e07-871c-c27f333eff76",
   "metadata": {},
   "source": [
    "- Polynomial regression is a statistical model that uses a polynomial function to predict a dependent variable. The polynomial function can be of any degree, but it is most commonly used for quadratic or cubic polynomials.\n",
    "\n",
    "- Linear regression is a special case of polynomial regression where the degree of the polynomial is 1. This means that the linear regression model is a straight line.\n",
    "\n",
    "- The main difference between polynomial regression and linear regression is that polynomial regression can model non-linear relationships between the dependent and independent variables. This is because the polynomial function can take on a variety of shapes, including curves.\n",
    "- price = b0 + b1 * square_footage + b2 * square_footage ^ 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5c0e3-1ec0-4f6c-8d66-bcea97cd4c33",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35f677-cc6b-4747-8168-d53cafb0abf0",
   "metadata": {},
   "source": [
    "\n",
    "### Here are the advantages and disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "## Advantages of Polynomial Regression\n",
    "\n",
    "- Can model non-linear relationships: Polynomial regression can model non-linear relationships between the dependent and independent variables, while linear regression can only model linear relationships.\n",
    "- More flexible: Polynomial regression is more flexible than linear regression, because it can be used to model a wider variety of relationships.\n",
    "Better accuracy: Polynomial regression can often provide better accuracy than linear regression, especially for non-linear relationships.\n",
    "Disadvantages of Polynomial Regression\n",
    "\n",
    "More complex: Polynomial regression is more complex than linear regression, both to implement and to interpret.\n",
    "More prone to overfitting: Polynomial regression is more prone to overfitting than linear regression, especially if the degree of the polynomial is too high.\n",
    "When to use polynomial regression\n",
    "\n",
    "Polynomial regression is a good choice when the relationship between the dependent and independent variables is non-linear. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
