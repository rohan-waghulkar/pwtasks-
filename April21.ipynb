{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd27e1e-617c-462c-b8ad-faa24b1cc376",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70e1b2-b723-4b9c-bff3-8243db9ca492",
   "metadata": {},
   "source": [
    "\n",
    "#### The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is how they calculate the distance between two data points.\n",
    "\n",
    "- Euclidean distance is the square root of the sum of the squared differences between the corresponding features of two data points.\n",
    "- Manhattan distance is the sum of the absolute differences between the corresponding features of two data points.\n",
    "#### This difference in calculation can affect the performance of a KNN classifier or regressor in a few ways:\n",
    "\n",
    "- Euclidean distance is more sensitive to outliers. Outliers are data points that are significantly different from the other data points in the dataset. Because Euclidean distance takes into account the magnitude of the differences between features, it is more sensitive to outliers than Manhattan distance. This can lead to decreased performance for KNN models that use Euclidean distance, especially in the presence of outliers.\n",
    "- Manhattan distance is more robust to noise. Noise is small, random variations in the data. Because Manhattan distance only takes into account the absolute value of the differences between features, it is more robust to noise than Euclidean distance. This can lead to improved performance for KNN models that use Manhattan distance, especially in the presence of noise.\n",
    "- Euclidean distance is more computationally expensive to calculate. Calculating the square root of a number is more computationally expensive than calculating the absolute value of a number. This means that KNN models that use Euclidean distance can be slower to train and predict than KNN models that use Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f235d3-401e-4656-9330-c0537e51f5d3",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a14171-b7cf-47d1-8b3f-1915e1d99499",
   "metadata": {},
   "source": [
    "- Cross-validation: Cross-validation is a statistical technique that can be used to evaluate the performance of a machine learning model on a held-out test set. To use cross-validation to determine the optimal k value, you would split your dataset into training and test sets. Then, you would train a KNN model with different values of k on the training set and evaluate its performance on the test set. The value of k that gives the best performance on the test set would be considered the optimal value of k.\n",
    "- Elbow method: The elbow method is a graphical technique that can be used to determine the optimal k value. To use the elbow method, you would plot the accuracy of the KNN model as a function of k. The elbow is the point where the accuracy starts to plateau. This is a good indication of the optimal value of k.\n",
    "- Trial and error: You can also try different values of k and see which value gives the best performance on your validation set. This is a less rigorous method than cross-validation or the elbow method, but it can be effective if you do not have a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8da059-835a-43a6-a61d-4cc08b70393b",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72c505-a275-4874-a7f3-03b1f68e75e8",
   "metadata": {},
   "source": [
    "#### Here are some of the factors that can affect the performance of KNN depending on the choice of distance metric:\n",
    "\n",
    "- Outliers: Some distance metrics are more sensitive to outliers than others. \n",
    "- Noise: Some distance metrics are more robust to noise than others. \n",
    "- Data distribution: The distribution of the data can also affect the performance of KNN depending on the choice of distance metric. For example, if the data is normally distributed, the Euclidean distance is a good choice. If the data is not normally distributed, it may be better to choose a different distance metric, such as the Manhattan distance or the cosine similarity.\n",
    "\n",
    "#### Here are some situations where you might choose one distance metric over another:\n",
    "- Euclidean distance works well when the data features are continuous and have similar units of measurement. It assumes that all dimensions are equally important.\n",
    "-  Manhattan distance is suitable when features have different units or when you want to emphasize differences in individual dimensions.It is also a good choice for data that contains outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f2be5-2fea-43d3-96d4-99e3b02166b0",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd679a64-bf69-48e3-a9fd-13d84b9885bf",
   "metadata": {},
   "source": [
    "1. n_neighbors : The number of nearest neighbors to consider when making a prediction.\n",
    "2. p : Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n",
    "3. Weighting scheme: The way in which the distance between data points is weighted when making a prediction.\n",
    "\n",
    "k is the most important hyperparameter in KNN. A larger value of k will make the model more robust to noise, but it can also lead to overfitting. A smaller value of k will make the model more sensitive to noise, but it can also help to prevent overfitting.\n",
    "\n",
    "Distance metric can also have a significant impact on the performance of KNN. The Euclidean distance metric is the most commonly used, but other distance metrics, such as the Manhattan distance and the cosine similarity metric, can be more effective for certain types of data.\n",
    "\n",
    "Weighting scheme can also affect the performance of KNN. The most common weighting scheme is the uniform weighting scheme, where all of the nearest neighbors are given equal weight. Other weighting schemes, such as the distance weighting scheme, can be more effective for certain types of data.\n",
    "\n",
    "To tune the hyperparameters of a KNN model, we can use a grid search or a random search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c4867-3207-4974-aeab-4791406f1306",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f0a9e-ec90-4a52-a402-dee7a5fedd49",
   "metadata": {},
   "source": [
    "The size of the training set has a significant impact on the performance of a KNN classifier or regressor. A larger training set will generally lead to better performance, as the model will have more data to learn from. However, there are a few things to keep in mind when increasing the size of the training set:\n",
    "\n",
    "- Computational complexity: KNN is a computationally expensive algorithm, especially for large training sets. It is important to consider the computational resources that are available when deciding how large to make the training set.\n",
    "- Overfitting: It is also important to be aware of the risk of overfitting when using a large training set. Overfitting occurs when the model learns the training data too well and is no longer able to generalize well to new data. To avoid overfitting, it is important to use regularization techniques and to evaluate the model on a held-out test set.\n",
    "#### Here are some techniques that can be used to optimize the size of the training set:\n",
    "\n",
    "- Use a validation set: A validation set is a set of data that is used to evaluate the performance of the model during training. By monitoring the performance of the model on the validation set, you can avoid overfitting and identify the optimal size of the training set.\n",
    "- Use a sampling technique: If the training set is very large, you can use a sampling technique to create a smaller subset of the data to train the model. This can help to reduce the computational complexity of the training process without significantly sacrificing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f8743-73f1-475f-aa14-e27f74e3dfff",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b872ab-bde5-4368-ab63-7b223ac00a4c",
   "metadata": {},
   "source": [
    "#### Here are some potential drawbacks of using KNN:\n",
    "\n",
    "- Computational complexity: KNN can be computationally expensive, especially for large training sets. This is because KNN needs to calculate the distance between the new data point and all of the data points in the training set.\n",
    "- Sensitivity to noise: KNN is sensitive to noise in the data. This is because KNN makes predictions based on the nearest neighbors of the new data point, and if those neighbors are noisy, the prediction will also be noisy.\n",
    "- Overfitting: KNN is prone to overfitting, especially for small training sets. This is because KNN can learn the training data too well and is no longer able to generalize well to new data.\n",
    "#### Here are some ways to overcome these drawbacks:\n",
    "\n",
    "- Computational complexity: To reduce the computational complexity of KNN, you can use a sampling technique to create a smaller subset of the data to train the model. You can also use a dimensionality reduction technique to reduce the number of features in the data.\n",
    "- Sensitivity to noise: To reduce the sensitivity of KNN to noise, you can use a distance metric that is robust to noise, such as the Manhattan distance. You can also use a weighting scheme that gives less weight to noisy neighbors.\n",
    "- Overfitting: To reduce overfitting, you can use regularization techniques, such as L1 regularization and L2 regularization. You can also evaluate the model on a held-out test set and stop training early if the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0efbcc-51e0-4249-a5b0-ef919c0b17ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
