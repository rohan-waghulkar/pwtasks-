{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343eb7bf-67ab-4966-99f9-e76dade448be",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76fc40-9848-4545-898b-cceb0014175a",
   "metadata": {},
   "source": [
    "The K-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It works by finding the k most similar examples in the training data to a new data point and then predicting the class or value of the new data point based on the classes or values of the k most similar examples.\n",
    "\n",
    "KNN is a non-parametric algorithm, which means that it does not make any assumptions about the underlying distribution of the data. This makes it a good choice for problems where the data is not well-understood or where the distribution of the data is likely to change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a785e-48a3-4c2d-8033-661193587d04",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ea0a4-ca52-4604-8eac-3f4e63b47533",
   "metadata": {},
   "source": [
    "Cross-validation : Most common technique used for finding k value is  cross-validation. we can use it  to evaluate the performance of different values of K on your training data. \n",
    "\n",
    "Trial and error. You can try different values of K and see which value gives the best performance on your validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85861b-6a02-486d-b75d-0a7796e84854",
   "metadata": {},
   "source": [
    "## Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43cf46-b303-46b9-940b-5f554bd50980",
   "metadata": {},
   "source": [
    "KNN classifier :\n",
    "- purpose : In KNN classifier regressor the goal is to identify that to which class the data point belongs to.\n",
    "- Working : The algorithm calculate the distance between the given data point and k nearest neighbors. and then it assign the lebal to the dat point which has majority in the k neighbors.\n",
    "\n",
    "KNN Regressor :\n",
    "- purpose : In KNN Regressor the goal is to predict continuous numeric value.\n",
    "- working The algorithm calculates the distance between the given data point and k nearest neighbors. and then calculates the average of  target values associated with these neighbors as an final prediction of the new data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87ef22-491f-4390-b4ec-23a99b60b6f0",
   "metadata": {},
   "source": [
    "## Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3f468-9b1c-4123-84aa-1a97e744d950",
   "metadata": {},
   "source": [
    "#### For classification tasks, common performance metrics include:\n",
    "\n",
    "- Accuracy: The percentage of correctly classified data points.\n",
    "- Precision: The percentage of positive predictions that are correct.\n",
    "- Recall: The percentage of positive data points that are correctly predicted.\n",
    "- F1 score: A harmonic mean of precision and recall.\n",
    "\n",
    "#### For regression tasks, common performance metrics include:\n",
    "\n",
    "- Mean squared error (MSE): The average squared difference between the predicted values and the actual values.\n",
    "- Mean absolute error (MAE): The average absolute difference between the predicted values and the actual values.\n",
    "- R-squared: A measure of how well the predicted values fit the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be775ba1-1225-4a2b-8213-59f4aad233e6",
   "metadata": {},
   "source": [
    "## Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c10a36-f89e-4db5-a2e8-bef70fc4a850",
   "metadata": {},
   "source": [
    "\n",
    "The curse of dimensionality is a phenomenon that occurs in machine learning algorithms when the number of dimensions of the input data is high. In the context of KNN, the curse of dimensionality can lead to the following problems:\n",
    "\n",
    "- The distance between data points becomes less meaningful in high-dimensional spaces. This can make it difficult for KNN to find the k most similar data points to a new data point.\n",
    "- KNN becomes more sensitive to noise in high-dimensional spaces. This is because the distance between data points in high-dimensional spaces is more likely to be affected by small changes in the data.\n",
    "- KNN becomes more computationally expensive in high-dimensional spaces. This is because KNN needs to calculate the distance between the new data point and all of the data points in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbec9bf-82b0-4b59-bdfc-bf1f5581e86b",
   "metadata": {},
   "source": [
    "## Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95430e-6d3d-456f-9e2c-5f9c767f6680",
   "metadata": {},
   "source": [
    "- Remove Data Points:If a data point has missing values for multiple features or if the percentage of missing values is very high, you might choose to remove those data points from the dataset. This approach should be used with caution, as it can lead to a loss of information.\n",
    "- Use of Distance Metrics: Modify the distance metric: You can modify the distance metric used in KNN to handle missing values more effectively. \n",
    "- Mean, Median, Mode Imputation: You can replace missing values with the mean, median, or mode of the feature's non-missing values. This approach is simple and often works well when missing data is missing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4dd25-92c5-4315-87f3-97b14c105d70",
   "metadata": {},
   "source": [
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f242cd2-e397-4f15-86e8-62628cfd7aa1",
   "metadata": {},
   "source": [
    "#### KNN Classifier:\n",
    "\n",
    "Purpose: KNN classifier is used for classification tasks, where the goal is to assign data points to predefined classes or categories.\n",
    "\n",
    "Output: The output of a KNN classifier is a class label, indicating the predicted category or class for a given data point.\n",
    "\n",
    "Performance Metrics: KNN classifier is typically evaluated using classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "KNN classifier is suitable for problems where the target variable is categorical, such as image recognition (e.g., classifying images of animals), text classification (e.g., sentiment analysis), and spam email detection.\n",
    "\n",
    "#### KNN Regressor:\n",
    "\n",
    "Purpose: KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "Output: The output of a KNN regressor is a numeric value, which represents the predicted continuous target variable.\n",
    "\n",
    "Performance Metrics: KNN regressor is typically evaluated using regression metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared (RÂ²).\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "KNN regressor is suitable for problems where the target variable is continuous, such as predicting housing prices based on features like square footage, predicting stock prices, or estimating a person's age based on various attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe4651-f3ba-419a-9d09-a97a89656ee1",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534163d6-eb80-4f86-b425-fd8cf46c8567",
   "metadata": {},
   "source": [
    "#### Strengths of KNN:\n",
    "\n",
    "- Simple and easy to understand: KNN is a relatively simple algorithm to understand, even for beginners to machine learning.\n",
    "- Effective for both linear and non-linear problems: KNN can be used to solve both linear and non-linear problems without the need for any prior assumptions about the underlying data distribution.\n",
    "- Can handle multi-class classification problems: KNN can be used to solve multi-class classification problems, where the data points can be classified into more than two categories.\n",
    "#### Weaknesses of KNN:\n",
    "\n",
    "- Sensitive to the choice of K: The performance of KNN is highly dependent on the choice of the K hyperparameter. A value of K that is too small can lead to overfitting, while a value of K that is too large can lead to underfitting.\n",
    "- Can be computationally expensive for large datasets: KNN can be computationally expensive for large datasets, as it needs to calculate the distance between the new data point and all of the data points in the training set.\n",
    "- Can be overfitting to the training data: KNN is prone to overfitting, especially for small datasets.\n",
    "#### How to address the weaknesses of KNN:\n",
    "\n",
    "- Choose the K hyperparameter carefully: There are a few different methods for choosing the K hyperparameter, such as cross-validation and the elbow method. It is important to experiment with different values of K to find the value that gives the best performance on the test set.\n",
    "- Use dimensionality reduction techniques: Dimensionality reduction techniques, such as PCA and t-SNE, can be used to reduce the number of features in the data, which can help to improve the performance and computational efficiency of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bd181-4e87-4dfc-8a0d-c02f58308d1f",
   "metadata": {},
   "source": [
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be314f0f-0684-453a-bde9-167dc16b3060",
   "metadata": {},
   "source": [
    " Euclidean distance is the square root of the sum of the squared differences between the corresponding features of two data points. It is the most common distance metric used in KNN because it is simple to calculate and has a number of desirable properties, such as symmetry and triangle inequality.\n",
    "\n",
    "Manhattan distance is the sum of the absolute differences between the corresponding features of two data points. It is also known as the L1 distance or the city block distance. Manhattan distance is less commonly used in KNN than Euclidean distance, but it can be more effective for data with outliers or features with different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84920f-0272-4bcc-bac9-be3b4226451e",
   "metadata": {},
   "source": [
    "## Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f39384-8de5-4221-8ebf-8ed04c6771e6",
   "metadata": {},
   "source": [
    "The primary role of feature scaling in KNN is to ensure that all features contribute equally to the distance calculations between data points. Here's why feature scaling is important in KNN:\n",
    "\n",
    "Distance Metric: KNN relies on distance metrics, such as Euclidean distance, Manhattan distance, or others, to measure the similarity between data points. These distance metrics are sensitive to the scale of the features. Features with larger scales can dominate the distance calculations, potentially causing the algorithm to be biased toward those features.\n",
    "\n",
    "Normalization of Features: Feature scaling normalizes the range of each feature so that they all have a similar scale. This ensures that no single feature has a disproportionate influence on the distance computation. Scaling transforms the features to a common scale, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Improved Model Performance: When features are scaled appropriately, KNN is more likely to find neighbors that are genuinely similar in terms of patterns and relationships among features. This leads to more accurate and robust predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4191c-0217-4ecc-9f09-20d1771d28a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
