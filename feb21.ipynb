{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01efd2bf-01a2-4f21-98c6-627ff2c77794",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91f242-00e6-45c5-9dc6-e0d238e300dd",
   "metadata": {},
   "source": [
    "#### The web scrapping is the process of automatically extracting the data from the website by writing few lines of code \n",
    "### Use of Web Scrapping :\n",
    "#### Business uses : Companies uses scraping to collect the data and then the data is used to some strategic decisions for the company.\n",
    "#### Research : Researchers can use web scraping to collect data for academic studies, market research, and social science research. \n",
    "#### Content Aggregation: Web scraping can be used to collect news articles, blog posts, and other online content for aggregation and distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0ec29-ab64-47eb-9cfe-2e54315d056f",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058be3e-a42f-49a5-aba6-981dbf687f39",
   "metadata": {},
   "source": [
    "#### Web Scraping Tools: There are various tools available for web scraping, such as Scrapy, Beautiful Soup, and Selenium. These tools use programming languages like Python to automate the process of extracting data from websites. They are more efficient than manual scraping and can handle large amounts of data.\n",
    "\n",
    "#### APIs: Many websites provide APIs (Application Programming Interfaces) that allow users to access their data in a structured format. APIs are a convenient and efficient way to extract data, as they are designed for this purpose and provide direct access to the data.\n",
    "\n",
    "#### Crawlers: Crawlers are automated scripts that systematically browse the web and collect data from websites. They are similar to web scraping tools but are designed to collect data on a large scale, often for search engines or data mining purposes.\n",
    "\n",
    "#### Scraping Services: There are also companies that offer web scraping services, which can be useful for businesses or individuals who do not have the technical skills or resources to conduct web scraping themselves. These services can be expensive, but they provide a convenient and efficient way to collect data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7dc6c-f99c-4ede-ae4f-673aa14341f5",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959324e-b7d1-47be-b695-97adb7d77353",
   "metadata": {},
   "source": [
    "#### Beautiful Soup is a library in the pythin that allow programmer to extract the data from the website i.e Web Scrapping\n",
    "### Uses\n",
    "#### Data Extraction : It is used for extracting data from html and xml documents \n",
    "#### Data Cleaning : Beautiful Soup can be used to clean up the extracted data by removing unwanted tags and attributes, replacing special characters, and formatting the data for further processing.\n",
    "#### Automation: Beautiful Soup can be used to automate the web scraping process by navigating through web pages, following links, and extracting data from multiple pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb075b13-cc43-4111-937d-e6594d77686c",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc532757-00da-40f5-aefd-9da496181729",
   "metadata": {},
   "source": [
    "#### It is commonly used for building web applications and APIs. Flask is a popular choice for web scraping projects because it provides a convenient and flexible way to build web applications\n",
    "#### we used the flask in our project to display the web scraping result as a webpage and also to take input that a user want to search for . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f2c54-e42e-4904-b9c6-cecd3c1821f2",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb39667-5786-465b-91c6-9283aa27e36f",
   "metadata": {},
   "source": [
    "#### In our project we have used two services of AWS CodePipeline and Elastic BeanStalk\n",
    "### CodePipeline:\n",
    "####       It provides a way to create automated workflows that move code changes from development to production. With CodePipeline, you can create a pipeline that automatically builds your code, tests it, and deploys it to Elastic Beanstalk.\n",
    "### Elastic BeanStalk:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
