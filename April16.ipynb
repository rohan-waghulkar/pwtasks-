{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f946135-1b88-46a4-8731-68dcd873b8a3",
   "metadata": {},
   "source": [
    "## Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340a6fc-812d-4979-bc87-31aca969e7eb",
   "metadata": {},
   "source": [
    "Boosting is type of ensemble techique in machine learning used to increase the performance of the model by reducing Bias and variance .\n",
    "It is a sequential ensemble method that builds a strong model from a set of week models.\n",
    "\n",
    "most common boosting algorithms are AdaBoost,GradientBoost and xgBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea7f66-aa1c-4d4f-a6f9-d275bdf1c027",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4eb6f-5d8f-4c0c-8b96-99bd04269271",
   "metadata": {},
   "source": [
    "##### Advantages :\n",
    "- reduces bias as well as variance .\n",
    "- reduces overfitting problem.\n",
    "- imporoves the accuracy of the model.\n",
    "##### Limitations:\n",
    "- requires lot of computational time.\n",
    "- sensitive to the choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf51b18-19fb-45ab-bdf9-6f487768a4fb",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45299ee-578f-46fa-9aac-186c2b328c49",
   "metadata": {},
   "source": [
    "1. week learner is trained on the entire training dataset.\n",
    "2. Then the error of the week learner is calculated.\n",
    "3. The weight of the training data is updated. more weight is aasigned to the datapoints that we misclassified.\n",
    "4. next week learner is trained on the updated training dataset.\n",
    "5. these step are repeated untill desired number of week learner have been trained.\n",
    "6. the prediction of the week learners are combined for a single strong learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1ad66-30a3-4848-827e-879fea1af699",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba094662-766e-411b-ac5d-b64338a3dd3f",
   "metadata": {},
   "source": [
    "1. AdaBoost: AdaBoost is the most common boosting algorithm. It works by iteratively training weak classifiers and assigning higher weights to misclassified instances. This forces the subsequent weak classifiers to focus on the most difficult instances to classify.\n",
    "\n",
    "2. Gradient Boosting: Gradient Boosting works by iteratively adding weak classifiers that minimize the gradient of the loss function. This is a more efficient way to train boosting models than AdaBoost.\n",
    "\n",
    "3. XGBoost: XGBoost is a more advanced version of Gradient Boosting that uses a number of techniques to improve performance, such as regularization and tree pruning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41657931-e0c5-4068-9f01-d2406842a08f",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0eb50e-d85b-42db-b30b-e2ec448413b6",
   "metadata": {},
   "source": [
    "n_estimators : Number of decision trees (weak learners) to be create.\n",
    "\n",
    "learning_rate :  This controls the impact of each tree on the final model.\n",
    "\n",
    "max_depth : It controls the levels of the decsion trees(weak learner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a06ac-36a9-443e-8975-e04dedfb2d3c",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd118e-3414-4d94-9d79-6f393ca27e08",
   "metadata": {},
   "source": [
    "Boosting algorithm combines weak learners to create a strong learner by sequentially training the weak learner on the data where each weak learner focuse on mistakes made by previous weak learner.This process is repeated untill desired level of accuracy is reached.\n",
    "\n",
    "Along with each weak leaner a learning rate parameter is assigned to control the impact of the weak learner on the final strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab83684-1ea7-4faa-86c6-95dbc4b6da5b",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3ffd7-d2bc-442d-a6fa-7842a2ff33cc",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm works as follows:\n",
    "\n",
    "- Initialize the weights of all data points to be equal.\n",
    "- Train a weak learner on the data, and calculate its error rate.\n",
    "- Calculate the weighted error of the weak learner, which is the error rate multiplied by the weights of the misclassified data points.\n",
    "- Update the weights of the data points, giving more weight to the data points that were misclassified by the weak learner.\n",
    "- Repeat steps 2-4 until a desired level of accuracy is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f994e-53bc-4258-b574-3309a6b859dd",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ffa97-a573-42bb-a7c4-530cad388c39",
   "metadata": {},
   "source": [
    "    L(y, f(x)) = exp(-yf(x))\n",
    "where y is the true label and f(x) is the prediction of the model. The exponential loss function is zero when the prediction is correct and increases exponentially as the prediction becomes more incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f7b80-47a6-43b9-8616-0b4c79cfd7ae",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f953a-d157-426d-8a57-444674cb5c24",
   "metadata": {},
   "source": [
    " AdaBoost updates the weights of the training examples based on the weighted error of the weak learner. The idea is to increase the weights of the misclassified examples so that they become more important in the next iteration. The formula to update the weights is as follows:\n",
    "\n",
    "For misclassified examples:\n",
    "\n",
    "New Weight = Old Weight * e^(α)\n",
    "For correctly classified examples:\n",
    "\n",
    "New Weight = Old Weight * e^(-α)\n",
    "Where:\n",
    "\n",
    "Old Weight is the weight of the example before the update.\n",
    "\n",
    "α (alpha) is a coefficient that depends on the weighted error of the weak learner "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47296e1e-a209-4d14-ad0d-ab3e3816374f",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7401386-98a6-4360-8def-3507cab11e23",
   "metadata": {},
   "source": [
    "\n",
    "Increasing the number of estimators in the AdaBoost algorithm can have a number of effects on the model.\n",
    "\n",
    "1. Increased accuracy: In general, increasing the number of estimators will lead to an increase in the accuracy of the model. This is because each estimator is trained on a different subset of the data, and the combined predictions of the estimators will be more accurate than the predictions of any individual estimator.\n",
    "2. Reduced bias: Increasing the number of estimators can also help to reduce the bias of the model. \n",
    "3. Increased variance: increasing the number of estimators can also lead to an increase in the variance of the model. Variance is the tendency of the model to make different predictions for the same data point. This can happen if the estimators are not sufficiently different from each other.\n",
    "4. Overfitting: If the number of estimators is too high, the model may start to overfit the training data.This can lead to poor performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda32681-a4e9-4fb2-a089-ff700e78ac40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
